{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import ft_ngram_hashes  # This function is used to calculate hashes from ngrams to determine position in ngram matrix\n",
    "from gensim.models.fasttext_inner import compute_ngrams_bytes, compute_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.KeyedVectors.load(\"models/fasttext/wiki.ru_gensim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888423, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.vectors_ngrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные значения корректируются по коду ниже\n",
    "new_vocab_size = 80_000\n",
    "new_ngrams_size = 100_000  # Should be GCD of initial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New vocab\n",
    "Here we select the most frequent words in existing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"models/train_input.txt\", \"r\")\n",
    "\n",
    "# считываем все строки\n",
    "all_text = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[text for text in doc.split()] for doc in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обучение используется лишь для получения всех токинов по системе fasttext\n",
    "model = FastText(texts, vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4753"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Второй способ\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# file = open(\"models/train_input.txt\", \"r\")\n",
    "\n",
    "# # считываем все строки\n",
    "# all_text = file.readlines()\n",
    "# file.close()\n",
    "\n",
    "# tfidf = TfidfVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "# tfidf.fit(all_text)\n",
    "\n",
    "# list(tfidf.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4753/4753 [11:56<00:00,  6.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2652, 47530)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Заполним сначала словами встречающимися в тексте\n",
    "ntop = 10\n",
    "all_list_words = []\n",
    "not_in_list = []\n",
    "osn_word = 0\n",
    "add_word = 0\n",
    "for word in tqdm.tqdm(model.wv.index_to_key):\n",
    "    if word in ft.key_to_index:\n",
    "        all_list_words.append(word)\n",
    "        osn_word += 1\n",
    "    else:\n",
    "        not_in_list.append(word)\n",
    "        \n",
    "    list_words = list(ft.most_similar(word, ntop))    \n",
    "    for word, score in list_words:\n",
    "        if word in ft.key_to_index:\n",
    "            all_list_words.append(word)\n",
    "            add_word += 1\n",
    "        \n",
    "osn_word, add_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32213"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Уникальный слов всего, до добавления нграмм\n",
    "all_list_words = list(set(all_list_words))\n",
    "len(all_list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_ngramm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Добавляем нграммы слов, которых не было в словаре\n",
    "for word in not_in_list:\n",
    "    encoded_ngrams = compute_ngrams(word, 1, ft.max_n)\n",
    "    for ngram in encoded_ngrams:\n",
    "        if ngram in ft.key_to_index:\n",
    "            list_all_ngramm.append(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Добавляем нграммы остальных слов\n",
    "for word in all_list_words:\n",
    "    encoded_ngrams = compute_ngrams(word, 1, ft.max_n)\n",
    "    for ngram in encoded_ngrams:\n",
    "        if ngram in ft.key_to_index:\n",
    "            list_all_ngramm.append(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50532"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Минимальное количество необходимых нграмм\n",
    "#Данное знаечение или больше необходимо записать в переменную new_ngrams_size\n",
    "list_all_ngramm = list(set(list_all_ngramm))\n",
    "len(list_all_ngramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72778"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Минимальное количество слов в словаре, лучше чуть больше сделать\n",
    "#Данное знаечение или больше необходимо записать в переменную new_vocab_size\n",
    "all_list_words.extend(list_all_ngramm)\n",
    "all_list_words = list(set(all_list_words))\n",
    "len(all_list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72778"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Итоговый список всех слов\n",
    "len(all_list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72778"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Получим индексы слов из словаря\n",
    "list_ft_index_word = []\n",
    "for word in all_list_words:\n",
    "    ft_index = ft.key_to_index[word]\n",
    "    list_ft_index_word.append(ft_index)\n",
    "list_check_word = all_list_words\n",
    "del(all_list_words)\n",
    "len(list_ft_index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сортируем список индексов слов в словаре ft\n",
    "list_ft_index_word = sorted(list_ft_index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7222"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Будет добавлено дополнительно частовстречаемых слов из основной модели\n",
    "add_vocab_size = max(0,new_vocab_size - len(list_ft_index_word))\n",
    "add_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Добавляем частовстречаемые слова в наш словарь\n",
    "for idx in range(new_vocab_size):\n",
    "    word = ft.index_to_key[idx]\n",
    "    if idx not in list_ft_index_word:\n",
    "        list_ft_index_word.append(idx)\n",
    "    if len(list_ft_index_word) >= new_vocab_size:\n",
    "        break\n",
    "len(list_ft_index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сортируем список по индексам в словаре ft еще раз\n",
    "list_ft_index_word = sorted(list_ft_index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ft_index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Создаем итоговый словрь\n",
    "top_vocab = {}\n",
    "list_top_vocab_vectors = []\n",
    "for idx in list_ft_index_word:\n",
    "    top_vocab[ft.index_to_key[idx]] = idx\n",
    "    list_top_vocab_vectors.append(ft.vectors_vocab[idx])\n",
    "#top_vocab\n",
    "top_vocab_vectors = np.array(list_top_vocab_vectors)\n",
    "top_vocab_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверим что количество полученных слов равно необходимому количеству слов\n",
    "assert new_vocab_size == len(list_ft_index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams remapping\n",
    "\n",
    "Ngram vectors are located by calculating hash of the ngram chars.\n",
    "We need to calculate new hashes for smaller matrix and map old vectors to a new ones.\n",
    "Since the size of the new matrix is smaller, there will be collisions.\n",
    "We are going to resolv them by calculating weighted sum of vectors of collided ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:01<00:00, 42109.74it/s]\n"
     ]
    }
   ],
   "source": [
    "new_to_old_buckets = defaultdict(set)\n",
    "old_hash_count = defaultdict(int)\n",
    "for word, vocab_word in tqdm.tqdm(top_vocab.items()):\n",
    "    old_hashes = ft_ngram_hashes(word, ft.min_n, ft.max_n, ft.bucket)\n",
    "    new_hashes = ft_ngram_hashes(word, ft.min_n, ft.max_n, new_ngrams_size)\n",
    "    \n",
    "    for old_hash in old_hashes:\n",
    "        old_hash_count[old_hash] += 1  # calculate frequency of ngrams for proper weighting\n",
    "        \n",
    "    for old_hash, new_hash in zip(old_hashes, new_hashes):\n",
    "        new_to_old_buckets[new_hash].add(old_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_like_init(n, dim=300, random_state=42):\n",
    "    rand_obj = np.random\n",
    "    rand_obj.seed(random_state)\n",
    "    lo, hi = -1.0 / dim, 1.0 / dim\n",
    "    new_ngrams = rand_obj.uniform(lo, hi, (n, dim)).astype(np.float32)\n",
    "    return new_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new FastText model instance\n",
    "new_ft = gensim.models.keyedvectors.FastTextKeyedVectors(\n",
    "    vector_size=ft.vector_size,\n",
    "    min_n=ft.min_n,\n",
    "    max_n=ft.max_n,\n",
    "    bucket=new_ngrams_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для новой модели, сбросим индексы, при этом оставим прежний порядок слов в словаре\n",
    "new_top_vocab = top_vocab.copy()\n",
    "for idx, key in enumerate(new_top_vocab.keys()):\n",
    "    new_top_vocab[key] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set shrinked vocab and vocab vector\n",
    "new_ft.vectors_vocab = top_vocab_vectors\n",
    "new_ft.vectors = new_ft.vectors_vocab\n",
    "new_ft.key_to_index = new_top_vocab\n",
    "new_ft.index_to_key = list(top_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96669"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Заполним полученное количество нграмм\n",
    "len(new_to_old_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ngrams_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ngram vectors\n",
    "new_ngrams = fasttext_like_init(n=new_ngrams_size, dim=ft.vectors_ngrams.shape[1], random_state=42)\n",
    "for new_hash, old_buckets in new_to_old_buckets.items():\n",
    "    total_sum = sum(old_hash_count[old_hash] for old_hash in old_buckets)\n",
    "    \n",
    "    new_vector = np.zeros(ft.vector_size, dtype=np.float32)\n",
    "    for old_hash in old_buckets:\n",
    "        weight = old_hash_count[old_hash] / total_sum\n",
    "        new_vector += ft.vectors_ngrams[old_hash] * weight\n",
    "    \n",
    "\n",
    "    new_ngrams[new_hash] = new_vector\n",
    "new_ft.vectors_ngrams = new_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastTextKeyedVectors at 0x7f8b0c384310>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Перезагрузим модель, чтобы произошло обновление таблицы вектров модели\n",
    "new_name_model = 'models/fasttext/shrinked_fasttext.model'\n",
    "new_ft.save(new_name_model)\n",
    "new_ft = gensim.models.KeyedVectors.load(new_name_model)\n",
    "new_ft "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating losses\n",
    "\n",
    "We may gain a view of accuracy losses by measuring similarity between original vocab vectors and new vectors recreated from shrink n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверим аналогичность новой модели путем запроса похожих слов, должна выдавать похожие результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('изоленты', 0.8522964715957642),\n",
       " ('изоленту', 0.8454886078834534),\n",
       " ('изолентой', 0.8099086284637451),\n",
       " ('изоле', 0.7382572293281555),\n",
       " ('полента', 0.6976317167282104),\n",
       " ('солента', 0.6515787243843079),\n",
       " ('изолы', 0.6471081376075745),\n",
       " ('полента\\xa0—', 0.6222870945930481),\n",
       " ('изол', 0.6166868805885315),\n",
       " ('перфолента', 0.6091929078102112)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.most_similar('изолента', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('изоленты', 0.8465209603309631),\n",
       " ('изоленту', 0.8347629308700562),\n",
       " ('изолентой', 0.8115509152412415),\n",
       " ('изоле', 0.7830286622047424),\n",
       " ('полента', 0.7714607119560242),\n",
       " ('изола', 0.7516038417816162),\n",
       " ('изол', 0.7508651614189148),\n",
       " ('тизола', 0.6680089831352234),\n",
       " ('тизол', 0.646018385887146),\n",
       " ('черизоле', 0.642540693283081)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ft.most_similar('изолента', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_vocab = list_check_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft.rank_by_centrality('изолента')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 100000 Similarity: 0.742301019452994\n"
     ]
    }
   ],
   "source": [
    "#Качество исходной модели составления из n-грамм\n",
    "sims = []\n",
    "old_word = ''\n",
    "for test_word in new_test_vocab:\n",
    "    str_word = test_word + '' + old_word\n",
    "    sim = ft.cosine_similarities(ft.get_vector(test_word) + ft.get_vector(old_word), [ft.get_vector(str_word)])\n",
    "    old_word = word\n",
    "    if not np.isnan(sim):\n",
    "        sims.append(sim)\n",
    "print(f\"size: {new_ngrams_size}\", \"Similarity:\", np.sum(sims) / len(new_test_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 100000 Similarity: 0.7300436022046498\n"
     ]
    }
   ],
   "source": [
    "#Качество новой модели составления из n-грамм\n",
    "sims = []\n",
    "old_word = ''\n",
    "for test_word in new_test_vocab:\n",
    "    str_word = test_word + '' + old_word\n",
    "    sim = ft.cosine_similarities(new_ft.get_vector(test_word) + new_ft.get_vector(old_word), [new_ft.get_vector(str_word)])\n",
    "    old_word = word\n",
    "    if not np.isnan(sim):\n",
    "        sims.append(sim)\n",
    "print(f\"size: {new_ngrams_size}\", \"Similarity:\", np.sum(sims) / len(new_test_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 100000 Similarity: 0.824769762153398\n"
     ]
    }
   ],
   "source": [
    "#Сравнение сходства двух моделей по основной методике\n",
    "sims = []\n",
    "for test_word in new_test_vocab:\n",
    "    sim = ft.cosine_similarities(ft.get_vector(test_word), [new_ft.get_vector(test_word)])\n",
    "    if not np.isnan(sim):\n",
    "        sims.append(sim)\n",
    "print(f\"size: {new_ngrams_size}\", \"Similarity:\", np.sum(sims) / len(new_test_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 100000 Similarity: 0.8457632393202617\n"
     ]
    }
   ],
   "source": [
    "#Сравнение сходства двух моделей по модифицированной методике\n",
    "sims = []\n",
    "old_word = ''\n",
    "for test_word in new_test_vocab:\n",
    "    str_word = test_word + '' + old_word\n",
    "    sim = ft.cosine_similarities(ft.get_vector(str_word), [new_ft.get_vector(str_word)])\n",
    "    old_word = word\n",
    "    if not np.isnan(sim):\n",
    "        sims.append(sim)\n",
    "print(f\"size: {new_ngrams_size}\", \"Similarity:\", np.sum(sims) / len(new_test_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
